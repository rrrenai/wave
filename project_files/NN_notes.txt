Use fixed point representation
* choose based on range of bits / biases
* balance accuracy and resources

Number representation
* signed magnitude or 2's complement
Signed magnitude
- first bit where 0 is pos and 1 is neg
- draw back is 2 representations for 0
- also when adding two sign magnitude numbers does not guarantee a sign magnitude result
2's complement
- take complement and add 1
- if MSB is 0 then pos and 1 then neg
- addition is very efficient cuz the result is another 2 comp
- but multiplication not so...

activation function
- non linear functions like sigmoid or hyperbolic challenging and may need 100s of neurons (LOTS of resources)
- SO, we can precalculate neuron values and store them in a Read-Only Memory (ROM) aka Look Up Table (LUT)
	- make sure you keep in mind the range of x

using IP cores
- built in IPs don't have enough flexibility and aren't portable

Designing a neuron
- some neurons may have hundreds of inputs to best utilize resources make things a little sequential...
	- keep it scalable tho
		- any number of inputs

1) input is multiplied by the weight memory from the pre-trained NN (weights are stored either RAM or ROM, weight is fetched by "interface and control logic")
2) the product will go into an accumulate function where the output will be added to an input
3) biases are also added
4) the sum is then passed thru a activation function where the threshold is stored in a ROM
5) the function is outputted